name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

permissions:
  contents: read
  security-events: write

jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r app/requirements.txt
        pip install pytest pytest-cov flake8
        
    - name: Run linting
      run: |
        flake8 app/src/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 app/src/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
        
    - name: Run unit tests
      run: |
        cd app
        python -m pytest tests/ -v --cov=src --cov-report=xml
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./app/coverage.xml
        flags: unittests
        name: codecov-umbrella

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
        
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v3
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    needs: [test, security-scan]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Log in to Docker Hub
      uses: docker/login-action@v3
      with:
        username: ${{ secrets.DOCKERHUB_USERNAME }}
        password: ${{ secrets.DOCKERHUB_TOKEN }}
        
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha
          
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: Dockerfile
        push: true
        tags: souravdixit04/demo_k8s_app_aws:main
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  terraform-plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    needs: [test, security-scan]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: "1.5.0"
        
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2
        
    - name: Terraform Init
      run: terraform init
      working-directory: terraform
      
    - name: Terraform Plan
      run: terraform plan -out=tfplan
      working-directory: terraform
      env:
        TF_VAR_ssh_public_key: ${{ secrets.SSH_PUBLIC_KEY }}
        
    - name: Rename lock file for upload
      run: cp terraform/.terraform.lock.hcl terraform/lockfile.hcl
        
    - name: Upload tfplan and lock file
      uses: actions/upload-artifact@v4
      with:
        name: tfplan
        path: |
          terraform/tfplan
          terraform/lockfile.hcl

  deploy-infrastructure:
    name: Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: [build-and-push, terraform-plan]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: "1.5.0"
        
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2
        
    - name: Download tfplan and lock file
      uses: actions/download-artifact@v4
      with:
        name: tfplan
        path: terraform
    - name: Rename lockfile back to .terraform.lock.hcl
      run: mv terraform/lockfile.hcl terraform/.terraform.lock.hcl
    - name: Verify downloaded files
      run: |
        echo "Downloaded artifact contents:"
        ls -la terraform/
        file terraform/.terraform.lock.hcl || echo "Lock file missing"
        
    - name: List files in terraform directory
      run: ls -la
      working-directory: terraform

    - name: Terraform Init (Apply)
      run: terraform init
      working-directory: terraform

    - name: Terraform Apply
      run: terraform apply -auto-approve tfplan
      working-directory: terraform
      env:
        TF_VAR_ssh_public_key: ${{ secrets.SSH_PUBLIC_KEY }}
        
    - name: Refresh Terraform State
      run: terraform refresh
      working-directory: terraform
        
    - name: Setup kubectl
      run: |
        # Install kubectl
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/
        
    - name: Get kubeconfig and setup cluster access
      run: |
        # Get cluster info from Terraform output
        cd terraform
        
        # Debug: List all available outputs
        echo "Available Terraform outputs:"
        terraform output
        
        # Get required outputs with error checking
        KUBERNETES_API_LB=$(terraform output -raw kubernetes_api_lb_dns 2>/dev/null || echo "")
        MASTER_INSTANCE_ID=$(terraform output -raw master_instance_id 2>/dev/null || echo "")
        
        echo "Kubernetes API Load Balancer: $KUBERNETES_API_LB"
        echo "Master Instance ID: $MASTER_INSTANCE_ID"
        
        # Validate outputs
        if [ -z "$KUBERNETES_API_LB" ]; then
          echo "ERROR: kubernetes_api_lb_dns output is empty or not found"
          exit 1
        fi
        
        if [ -z "$MASTER_INSTANCE_ID" ]; then
          echo "ERROR: master_instance_id output is empty or not found"
          echo "Trying to get instance ID from AWS directly..."
          
          # Try multiple approaches to get instance ID
          echo "Attempting to find master instance by tag..."
          MASTER_INSTANCE_ID=$(aws ec2 describe-instances \
            --filters "Name=tag:Name,Values=k3s-demo-cluster-master" \
            --query 'Reservations[0].Instances[0].InstanceId' \
            --output text 2>/dev/null || echo "")
          
          if [ -z "$MASTER_INSTANCE_ID" ] || [ "$MASTER_INSTANCE_ID" = "None" ]; then
            echo "Trying to find instance by cluster name tag..."
            MASTER_INSTANCE_ID=$(aws ec2 describe-instances \
              --filters "Name=tag:kubernetes.io/cluster/k3s-demo-cluster,Values=owned" \
              --query 'Reservations[0].Instances[0].InstanceId' \
              --output text 2>/dev/null || echo "")
          fi
          
          if [ -z "$MASTER_INSTANCE_ID" ] || [ "$MASTER_INSTANCE_ID" = "None" ]; then
            echo "Trying to find instance by instance type and state..."
            MASTER_INSTANCE_ID=$(aws ec2 describe-instances \
              --filters "Name=instance-type,Values=t3.micro" "Name=instance-state-name,Values=running" \
              --query 'Reservations[0].Instances[0].InstanceId' \
              --output text 2>/dev/null || echo "")
          fi
          
          if [ -z "$MASTER_INSTANCE_ID" ] || [ "$MASTER_INSTANCE_ID" = "None" ]; then
            echo "ERROR: Could not find master instance ID"
            echo "Available instances:"
            aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,Tags[?Key==`Name`].Value|[0],State.Name,InstanceType]' --output table
            exit 1
          fi
          
          echo "Found Master Instance ID: $MASTER_INSTANCE_ID"
        fi
        
        # Wait for SSM to be available on the instance
        echo "Waiting for SSM to be available on master node..."
        timeout 300 bash -c 'until aws ssm describe-instance-information --filters "Key=InstanceIds,Values=$MASTER_INSTANCE_ID" --query "InstanceInformationList[0].PingStatus" --output text | grep -q "Online"; do sleep 10; done'
        
        # Get kubeconfig using SSM
        echo "Retrieving kubeconfig from master node via SSM..."
        aws ssm start-session --target $MASTER_INSTANCE_ID --document-name AWS-StartInteractiveCommand --parameters 'command="sudo cat /etc/rancher/k3s/k3s.yaml"' > kubeconfig_raw || {
          echo "Failed to get kubeconfig via SSM. Trying alternative method..."
          # Alternative: Use SSM send-command
          aws ssm send-command \
            --instance-ids $MASTER_INSTANCE_ID \
            --document-name "AWS-RunShellScript" \
            --parameters 'commands=["sudo cat /etc/rancher/k3s/k3s.yaml"]' \
            --query 'Command.CommandId' \
            --output text > command_id.txt
          
          COMMAND_ID=$(cat command_id.txt)
          echo "Command ID: $COMMAND_ID"
          
          # Wait for command to complete
          sleep 10
          
          # Get command output
          aws ssm get-command-invocation \
            --command-id $COMMAND_ID \
            --instance-id $MASTER_INSTANCE_ID \
            --query 'StandardOutputContent' \
            --output text > kubeconfig
        }
        
        # If first method worked, extract the content
        if [ -f kubeconfig_raw ]; then
          # Extract the kubeconfig content from the SSM session output
          grep -A 1000 "apiVersion:" kubeconfig_raw > kubeconfig || {
            echo "Failed to extract kubeconfig from SSM output"
            exit 1
          }
        fi
        
        # Check if kubeconfig was retrieved successfully
        if [ ! -f kubeconfig ] || [ ! -s kubeconfig ]; then
          echo "ERROR: Could not retrieve kubeconfig from master node"
          echo "Master node may still be initializing. Please check the instance status."
          exit 1
        fi
        
        # Update kubeconfig to use Load Balancer DNS
        sed -i "s|server: https://127.0.0.1:6443|server: https://$KUBERNETES_API_LB:6443|g" kubeconfig
        
        export KUBECONFIG=./kubeconfig
        
        # Wait for cluster to be ready
        echo "Waiting for cluster to be ready..."
        timeout 300 bash -c 'until kubectl get nodes; do sleep 10; done'
        
        echo "Cluster is ready!"
        kubectl get nodes
        
    - name: Install and configure ArgoCD
      run: |
        cd terraform
        export KUBECONFIG=./kubeconfig
        
        echo "Installing ArgoCD..."
        
        # Create ArgoCD namespace
        kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -
        
        # Install ArgoCD
        kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
        
        # Wait for ArgoCD to be ready
        echo "Waiting for ArgoCD to be ready..."
        kubectl wait --for=condition=available --timeout=600s deployment/argocd-server -n argocd || {
          echo "ArgoCD server deployment failed to become ready. Checking logs..."
          kubectl logs -n argocd deployment/argocd-server --tail=20
          echo "Continuing with installation..."
        }
        
        # Scale down ArgoCD for resource efficiency
        echo "Scaling ArgoCD for resource efficiency..."
        kubectl scale deployment argocd-server -n argocd --replicas=1
        kubectl scale deployment argocd-repo-server -n argocd --replicas=1
        kubectl scale deployment argocd-application-controller -n argocd --replicas=1
        
        # Wait for scaled deployments to be ready
        kubectl wait --for=condition=available --timeout=300s deployment/argocd-server -n argocd
        kubectl wait --for=condition=available --timeout=300s deployment/argocd-repo-server -n argocd
        kubectl wait --for=condition=available --timeout=300s deployment/argocd-application-controller -n argocd
        
        echo "ArgoCD installation completed successfully!"
        
        # Get ArgoCD admin password
        ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d 2>/dev/null || echo "password-not-available-yet")
        echo "ArgoCD admin password: $ARGOCD_PASSWORD"
        
        # Save ArgoCD info
        echo "ArgoCD admin password: $ARGOCD_PASSWORD" > argocd-info.txt
        echo "ArgoCD UI will be available at: http://$KUBERNETES_API_LB:8080" >> argocd-info.txt
        
    - name: Deploy ArgoCD Application
      run: |
        cd terraform
        export KUBECONFIG=./kubeconfig
        
        echo "Deploying ArgoCD Application..."
        
        # Apply the ArgoCD application manifest
        kubectl apply -f ../argocd/application.yaml
        
        # Wait for the application to be synced
        echo "Waiting for ArgoCD application to sync..."
        kubectl wait --for=condition=healthy --timeout=300s application/python-app -n argocd || {
          echo "Application sync failed. Checking ArgoCD application status..."
          kubectl describe application python-app -n argocd
          echo "Continuing..."
        }
        
        echo "ArgoCD application deployment completed!"
        
    - name: Deploy application with Helm
      run: |
        cd terraform
        export KUBECONFIG=./kubeconfig
        
        # Install Helm
        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
        
        # Deploy the application
        echo "Deploying application with Helm..."
        helm install python-app ../helm/python-app \
          --set image.repository=souravdixit04/demo_k8s_app_aws \
          --set image.tag=main \
          --wait --timeout=300s
        
        echo "Application deployment completed!"
        
    - name: Verify deployment
      run: |
        cd terraform
        export KUBECONFIG=./kubeconfig
        
        echo "Verifying deployment..."
        kubectl get pods -A
        kubectl get svc -A
        
        # Check if ArgoCD is running
        echo "ArgoCD status:"
        kubectl get pods -n argocd
        
        # Check if application is running
        echo "Application status:"
        kubectl get pods -l app=python-app
        
    - name: Get cluster info
      run: |
        cd terraform
        terraform output -json > cluster-info.json
        
        # Add ArgoCD info to cluster info
        if [ -f argocd-info.txt ]; then
          echo "ArgoCD Information:" >> cluster-info.txt
          cat argocd-info.txt >> cluster-info.txt
        fi
        
    - name: Upload cluster info
      uses: actions/upload-artifact@v4
      with:
        name: cluster-info
        path: |
          terraform/cluster-info.json
          terraform/cluster-info.txt

  notify:
    name: Notify Deployment
    runs-on: ubuntu-latest
    needs: [deploy-infrastructure]
    if: always()
    
    steps:
    - name: Check for SLACK_WEBHOOK_URL secret
      run: |
        if [ -z "${{ secrets.SLACK_WEBHOOK_URL }}" ]; then
          echo "SLACK_WEBHOOK_URL is not set"
        else
          echo "SLACK_WEBHOOK_URL is set"
        fi 
    - name: Slack Notification
      uses: 8398a7/action-slack@v3
      with:
        status: success
        channel: '#new-channel'
        author_name: GitHub Actions
        github_token: ${{ secrets.GITHUB_TOKEN }}
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        REGISTRY: ghcr.io
        IMAGE_NAME: souravdixit04/demo_k8s_app_aws

