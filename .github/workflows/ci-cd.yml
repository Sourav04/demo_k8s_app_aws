name: CI/CD Pipeline

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]

env:
  REGISTRY: ghcr.io
  IMAGE_NAME: ${{ github.repository }}

permissions:
  contents: read
  security-events: write

jobs:
  test:
    name: Run Tests
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r app/requirements.txt
        pip install pytest pytest-cov flake8
        
    - name: Run linting
      run: |
        flake8 app/src/ --count --select=E9,F63,F7,F82 --show-source --statistics
        flake8 app/src/ --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
        
    - name: Run unit tests
      run: |
        cd app
        python -m pytest tests/ -v --cov=src --cov-report=xml
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./app/coverage.xml
        flags: unittests
        name: codecov-umbrella

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
        
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v3
      if: always()
      with:
        sarif_file: 'trivy-results.sarif'

  build-and-push:
    name: Build and Push Docker Image
    runs-on: ubuntu-latest
    needs: [test, security-scan]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      
    - name: Log in to Docker Hub
      uses: docker/login-action@v3
      with:
        username: ${{ secrets.DOCKERHUB_USERNAME }}
        password: ${{ secrets.DOCKERHUB_TOKEN }}
        
    - name: Extract metadata
      id: meta
      uses: docker/metadata-action@v5
      with:
        images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}
        tags: |
          type=ref,event=branch
          type=ref,event=pr
          type=semver,pattern={{version}}
          type=semver,pattern={{major}}.{{minor}}
          type=sha
          
    - name: Build and push Docker image
      uses: docker/build-push-action@v5
      with:
        context: .
        file: Dockerfile
        push: true
        tags: souravdixit04/demo_k8s_app_aws:main
        labels: ${{ steps.meta.outputs.labels }}
        cache-from: type=gha
        cache-to: type=gha,mode=max

  terraform-plan:
    name: Terraform Plan
    runs-on: ubuntu-latest
    needs: [test, security-scan]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: "1.5.0"
        
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2
        
    - name: Terraform Init
      run: terraform init
      working-directory: terraform
      
    - name: Terraform Plan
      run: terraform plan -out=tfplan
      working-directory: terraform
      env:
        TF_VAR_ssh_public_key: ${{ secrets.SSH_PUBLIC_KEY }}
        
    - name: Rename lock file for upload
      run: cp terraform/.terraform.lock.hcl terraform/lockfile.hcl
        
    - name: Upload tfplan and lock file
      uses: actions/upload-artifact@v4
      with:
        name: tfplan
        path: |
          terraform/tfplan
          terraform/lockfile.hcl

  deploy-infrastructure:
    name: Deploy Infrastructure
    runs-on: ubuntu-latest
    needs: [build-and-push, terraform-plan]
    if: github.event_name == 'push' && github.ref == 'refs/heads/main'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      
    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: "1.5.0"
        
    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: us-west-2
        
    - name: Download tfplan and lock file
      uses: actions/download-artifact@v4
      with:
        name: tfplan
        path: terraform
    - name: Rename lockfile back to .terraform.lock.hcl
      run: mv terraform/lockfile.hcl terraform/.terraform.lock.hcl
    - name: Verify downloaded files
      run: |
        echo "Downloaded artifact contents:"
        ls -la terraform/
        file terraform/.terraform.lock.hcl || echo "Lock file missing"
        
    - name: List files in terraform directory
      run: ls -la
      working-directory: terraform

    - name: Terraform Init (Apply)
      run: terraform init
      working-directory: terraform

    - name: Terraform Apply
      run: terraform apply -auto-approve tfplan
      working-directory: terraform
      env:
        TF_VAR_ssh_public_key: ${{ secrets.SSH_PUBLIC_KEY }}
        
    - name: Refresh Terraform State
      run: terraform refresh
      working-directory: terraform
        
    - name: Debug Terraform State
      run: |
        echo "=== Terraform State Debug ==="
        echo "Current working directory: $(pwd)"
        echo "Terraform state file exists: $(ls -la terraform.tfstate* 2>/dev/null || echo 'No state file found')"
        echo ""
        echo "=== Terraform Outputs ==="
        terraform output || echo "No outputs available"
        echo ""
        echo "=== Terraform State (JSON) ==="
        terraform show -json | jq '.values.root_module.resources[] | select(.type == "aws_instance") | {name: .name, id: .values.id, tags: .values.tags}' || echo "Failed to parse state"
        echo ""
        echo "=== AWS Instances ==="
        aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,Tags[?Key==`Name`].Value|[0],State.Name,InstanceType]' --output table || echo "Failed to list instances"
      working-directory: terraform
        
    - name: Install required tools
      run: |
        # Install jq for JSON parsing
        sudo apt-get update
        sudo apt-get install -y jq
        
    - name: Setup kubectl
      run: |
        # Install kubectl
        curl -LO "https://dl.k8s.io/release/$(curl -L -s https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl"
        chmod +x kubectl
        sudo mv kubectl /usr/local/bin/
        
    - name: Get kubeconfig and setup cluster access
      run: |
        # Get cluster info from Terraform output
        cd terraform
        
        # Debug: List all available outputs
        echo "Available Terraform outputs:"
        terraform output
        
        # Get required outputs with error checking
        KUBERNETES_API_LB=$(terraform output -raw kubernetes_api_lb_dns 2>/dev/null || echo "")
        MASTER_INSTANCE_ID=$(terraform output -raw master_instance_id 2>/dev/null || echo "")
        MASTER_PRIVATE_IP=$(terraform output -raw master_node_ip 2>/dev/null || echo "")
        
        echo "Kubernetes API Load Balancer: $KUBERNETES_API_LB"
        echo "Master Instance ID: $MASTER_INSTANCE_ID"
        echo "Master Private IP: $MASTER_PRIVATE_IP"
        
        # Validate outputs
        if [ -z "$KUBERNETES_API_LB" ]; then
          echo "ERROR: kubernetes_api_lb_dns output is empty or not found"
          exit 1
        fi
        
        if [ -z "$MASTER_INSTANCE_ID" ]; then
          echo "ERROR: master_instance_id output is empty or not found"
          echo "Trying to get instance ID from Terraform state file..."
          
          # Try to get instance ID from Terraform state
          MASTER_INSTANCE_ID=$(terraform show -json | jq -r '.values.root_module.resources[] | select(.type == "aws_instance" and .name == "k3s_master") | .values.id' 2>/dev/null || echo "")
          
          if [ -z "$MASTER_INSTANCE_ID" ] || [ "$MASTER_INSTANCE_ID" = "null" ]; then
            echo "Trying to get instance ID from AWS directly..."
            
            # Try multiple approaches to get instance ID
            echo "Attempting to find master instance by tag..."
            MASTER_INSTANCE_ID=$(aws ec2 describe-instances \
              --filters "Name=tag:Name,Values=k3s-demo-cluster-master" \
              --query 'Reservations[0].Instances[0].InstanceId' \
              --output text 2>/dev/null || echo "")
            
            if [ -z "$MASTER_INSTANCE_ID" ] || [ "$MASTER_INSTANCE_ID" = "None" ]; then
              echo "Trying to find instance by cluster name tag..."
              MASTER_INSTANCE_ID=$(aws ec2 describe-instances \
                --filters "Name=tag:kubernetes.io/cluster/k3s-demo-cluster,Values=owned" \
                --query 'Reservations[0].Instances[0].InstanceId' \
                --output text 2>/dev/null || echo "")
            fi
            
            if [ -z "$MASTER_INSTANCE_ID" ] || [ "$MASTER_INSTANCE_ID" = "None" ]; then
              echo "Trying to find instance by instance type and state..."
              MASTER_INSTANCE_ID=$(aws ec2 describe-instances \
                --filters "Name=instance-type,Values=t3.micro" "Name=instance-state-name,Values=running" \
                --query 'Reservations[0].Instances[0].InstanceId' \
                --output text 2>/dev/null || echo "")
            fi
            
            if [ -z "$MASTER_INSTANCE_ID" ] || [ "$MASTER_INSTANCE_ID" = "None" ]; then
              echo "ERROR: Could not find master instance ID"
              echo "Available instances:"
              aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,Tags[?Key==`Name`].Value|[0],State.Name,InstanceType]' --output table
              exit 1
            fi
          fi
          
          echo "Found Master Instance ID: $MASTER_INSTANCE_ID"
        fi
        
        # Wait for SSM to be available on the instance
        echo "Waiting for SSM to be available on master node..."
        
        # Double-check that we have a valid instance ID
        if [ -z "$MASTER_INSTANCE_ID" ] || [ "$MASTER_INSTANCE_ID" = "None" ] || [ "$MASTER_INSTANCE_ID" = "null" ]; then
          echo "ERROR: MASTER_INSTANCE_ID is still empty or invalid: '$MASTER_INSTANCE_ID'"
          echo "Available instances in AWS:"
          aws ec2 describe-instances --query 'Reservations[*].Instances[*].[InstanceId,Tags[?Key==`Name`].Value|[0],State.Name,InstanceType]' --output table
          exit 1
        fi
        
        echo "Using Master Instance ID: '$MASTER_INSTANCE_ID'"
        
        # Test SSM connectivity first
        echo "Testing SSM connectivity..."
        aws ssm describe-instance-information --filters "Key=InstanceIds,Values=$MASTER_INSTANCE_ID" --query "InstanceInformationList" --output table || {
          echo "SSM connectivity test failed. Checking instance status..."
          aws ec2 describe-instances --instance-ids "$MASTER_INSTANCE_ID" --query 'Reservations[0].Instances[0].[State.Name,IamInstanceProfile.Arn]' --output table
          exit 1
        }
        
        # Export the variable for the timeout command
        export MASTER_INSTANCE_ID
        
        echo "Waiting for SSM to be fully ready..."
        timeout 300 bash -c '
          while true; do
            STATUS=$(aws ssm describe-instance-information --filters "Key=InstanceIds,Values=$MASTER_INSTANCE_ID" --query "InstanceInformationList[0].PingStatus" --output text 2>/dev/null || echo "Unknown")
            echo "SSM Status: $STATUS"
            if [ "$STATUS" = "Online" ]; then
              echo "SSM is ready!"
              break
            fi
            sleep 10
          done
        '
        
        # Get kubeconfig using SSM
        echo "Retrieving kubeconfig from master node via SSM..."
        
        # Ensure the variable is still available
        echo "Using Master Instance ID: '$MASTER_INSTANCE_ID'"
        
        # Try the send-command method first (more reliable)
        echo "Attempting to get kubeconfig via SSM send-command..."
        COMMAND_ID=$(aws ssm send-command \
          --instance-ids "$MASTER_INSTANCE_ID" \
          --document-name "AWS-RunShellScript" \
          --parameters 'commands=["sudo cat /etc/rancher/k3s/k3s.yaml"]' \
          --query 'Command.CommandId' \
          --output text)
        
        echo "Command ID: $COMMAND_ID"
        
        # Wait for command to complete
        echo "Waiting for command to complete..."
        sleep 15
        
        # Get command output
        echo "Retrieving command output..."
        aws ssm get-command-invocation \
          --command-id "$COMMAND_ID" \
          --instance-id "$MASTER_INSTANCE_ID" \
          --query 'StandardOutputContent' \
          --output text > kubeconfig
        
        # Check if kubeconfig was retrieved successfully
        if [ ! -f kubeconfig ] || [ ! -s kubeconfig ]; then
          echo "ERROR: Could not retrieve kubeconfig from master node"
          echo "Master node may still be initializing. Please check the instance status."
          exit 1
        fi
        
        echo "Original kubeconfig content:"
        cat kubeconfig
        echo "--- End of original kubeconfig ---"
        
        # Save original kubeconfig for fallback
        cp kubeconfig kubeconfig_original
        
        # Extract the token from the original kubeconfig
        echo "Extracting token from original kubeconfig..."
        echo "Original kubeconfig content:"
        cat kubeconfig
        echo "--- End of original kubeconfig ---"
        
        # Try multiple methods to extract the token
        TOKEN=""
        
        # Method 1: Look for token in user section
        TOKEN=$(grep -A 1 "token:" kubeconfig | tail -n 1 | tr -d ' ')
        
        # Method 2: If Method 1 failed, try different pattern
        if [ -z "$TOKEN" ]; then
          echo "Method 1 failed, trying Method 2..."
          TOKEN=$(awk '/token:/{getline; print $0}' kubeconfig | tr -d ' ')
        fi
        
        # Method 3: If Method 2 failed, try grep with context
        if [ -z "$TOKEN" ]; then
          echo "Method 2 failed, trying Method 3..."
          TOKEN=$(grep -A 5 "users:" kubeconfig | grep "token:" | head -1 | awk '{print $2}' | tr -d '"')
        fi
        
        # Method 4: If all methods failed, get token directly from master node
        if [ -z "$TOKEN" ]; then
          echo "All local methods failed, getting token from master node..."
          COMMAND_ID=$(aws ssm send-command \
            --instance-ids "$MASTER_INSTANCE_ID" \
            --document-name "AWS-RunShellScript" \
            --parameters 'commands=["sudo cat /var/lib/rancher/k3s/server/node-token"]' \
            --query 'Command.CommandId' \
            --output text)
          
          sleep 10
          
          TOKEN=$(aws ssm get-command-invocation \
            --command-id "$COMMAND_ID" \
            --instance-id "$MASTER_INSTANCE_ID" \
            --query 'StandardOutputContent' \
            --output text | tr -d '\n\r')
        fi
        
        if [ -z "$TOKEN" ]; then
          echo "ERROR: Could not extract token from kubeconfig or master node"
          echo "kubeconfig content:"
          cat kubeconfig
          echo "Trying to get token directly from master node file..."
          aws ssm send-command \
            --instance-ids "$MASTER_INSTANCE_ID" \
            --document-name "AWS-RunShellScript" \
            --parameters 'commands=["sudo cat /var/lib/rancher/k3s/server/node-token"]'
          exit 1
        fi
        
        echo "Token extracted: ${TOKEN:0:10}..."
        
        # Update kubeconfig to use Load Balancer DNS and disable TLS verification
        echo "Updating kubeconfig to use Load Balancer DNS: $KUBERNETES_API_LB"
        
        # Create a completely new kubeconfig with TLS verification disabled
        echo "Creating new kubeconfig with TLS verification disabled..."
        echo "apiVersion: v1" > kubeconfig_new
        echo "kind: Config" >> kubeconfig_new
        echo "clusters:" >> kubeconfig_new
        echo "- name: k3s-cluster" >> kubeconfig_new
        echo "  cluster:" >> kubeconfig_new
        echo "    server: https://KUBERNETES_API_LB_PLACEHOLDER:6443" >> kubeconfig_new
        echo "    insecure-skip-tls-verify: true" >> kubeconfig_new
        echo "contexts:" >> kubeconfig_new
        echo "- name: k3s-context" >> kubeconfig_new
        echo "  context:" >> kubeconfig_new
        echo "    cluster: k3s-cluster" >> kubeconfig_new
        echo "    user: k3s-user" >> kubeconfig_new
        echo "current-context: k3s-context" >> kubeconfig_new
        echo "users:" >> kubeconfig_new
        echo "- name: k3s-user" >> kubeconfig_new
        echo "  user:" >> kubeconfig_new
        echo "    token: \"TOKEN_PLACEHOLDER\"" >> kubeconfig_new
        
        # Replace placeholders with actual values
        sed -i "s/KUBERNETES_API_LB_PLACEHOLDER/$KUBERNETES_API_LB/g" kubeconfig_new
        sed -i "s/TOKEN_PLACEHOLDER/$TOKEN/g" kubeconfig_new
        
        # Use the new kubeconfig
        cp kubeconfig_new kubeconfig
        
        echo "New kubeconfig content:"
        cat kubeconfig
        echo "--- End of new kubeconfig ---"
        
        # Verify the token was properly set
        echo "Verifying token in kubeconfig..."
        KUBECONFIG_TOKEN=$(grep "token:" kubeconfig | head -1 | awk '{print $2}' | tr -d '"')
        if [ -z "$KUBECONFIG_TOKEN" ]; then
          echo "ERROR: Token is empty in kubeconfig"
          echo "Original token: ${TOKEN:0:10}..."
          echo "kubeconfig token field: $(grep 'token:' kubeconfig)"
          exit 1
        fi
        echo "Token in kubeconfig: ${KUBECONFIG_TOKEN:0:10}..."
        
        export KUBECONFIG=./kubeconfig
        
        # Export the Load Balancer DNS for the timeout command
        export KUBERNETES_API_LB
        
        # Wait for cluster to be ready
        echo "Waiting for cluster to be ready..."
        echo "Current kubeconfig content:"
        cat kubeconfig
        echo "--- End of kubeconfig ---"
        
        # Try using original kubeconfig first (it should have correct authentication)
        echo "Trying original kubeconfig with server URL update..."
        cp kubeconfig_original kubeconfig
        sed -i "s|server: https://127.0.0.1:6443|server: https://$KUBERNETES_API_LB:6443|g" kubeconfig
        
        # Add insecure-skip-tls-verify to cluster section and remove certificate authority
        sed -i '/cluster:/a\    insecure-skip-tls-verify: true' kubeconfig
        sed -i '/certificate-authority-data:/d' kubeconfig
        
        echo "Modified original kubeconfig content:"
        cat kubeconfig
        echo "--- End of modified original kubeconfig ---"
        
        timeout 300 bash -c '
          for i in {1..30}; do
            echo "Attempt $i: Testing cluster connectivity..."
            echo "Testing with verbose output:"
            kubectl get nodes --request-timeout=10s -v=6 2>&1 || {
              echo "kubectl failed with exit code $?"
              echo "Testing basic kubectl functionality:"
              kubectl version --client
              echo "Testing server connectivity:"
              curl -k -v https://$KUBERNETES_API_LB:6443/api 2>&1 | head -20
              echo "Cluster not ready yet, waiting 10 seconds..."
              sleep 10
            }
          done
        ' || {
          echo "ERROR: Cluster did not become ready within 5 minutes"
          echo "Final kubeconfig:"
          cat kubeconfig
          echo "Testing kubectl version:"
          kubectl version --client
          echo "Testing server connectivity:"
          curl -k -v https://$KUBERNETES_API_LB:6443/api 2>&1
          exit 1
        }
        
        echo "Cluster is ready!"
        kubectl get nodes
        
        echo "Note: Using Load Balancer DNS with TLS verification disabled for development."
        
        # Check Load Balancer and target group health
        echo "Checking Load Balancer and target group health..."
        echo "Load Balancer DNS: $KUBERNETES_API_LB"
        
        # First, check if k3s is running on the master node
        echo "Checking if k3s is running on master node..."
        aws ssm send-command \
          --instance-ids "$MASTER_INSTANCE_ID" \
          --document-name "AWS-RunShellScript" \
          --parameters 'commands=[
            "sudo systemctl status k3s",
            "sudo kubectl get nodes",
            "sudo kubectl get pods -A"
          ]' \
          --query 'Command.CommandId' \
          --output text > k3s_check_id.txt
        
        K3S_CHECK_ID=$(cat k3s_check_id.txt)
        sleep 15
        
        echo "k3s status on master node:"
        aws ssm get-command-invocation \
          --command-id "$K3S_CHECK_ID" \
          --instance-id "$MASTER_INSTANCE_ID" \
          --query 'StandardOutputContent' \
          --output text
        
        # Get Load Balancer ARN
        LB_ARN=$(aws elbv2 describe-load-balancers --names k3s-demo-cluster-api-lb --query 'LoadBalancers[0].LoadBalancerArn' --output text 2>/dev/null || echo "")
        if [ -n "$LB_ARN" ]; then
          echo "Load Balancer ARN: $LB_ARN"
          
          # Check target group health
          TG_ARN=$(aws elbv2 describe-target-groups --load-balancer-arn "$LB_ARN" --query 'TargetGroups[0].TargetGroupArn' --output text 2>/dev/null || echo "")
          if [ -n "$TG_ARN" ]; then
            echo "Target Group ARN: $TG_ARN"
            echo "Target Group Health:"
            aws elbv2 describe-target-health --target-group-arn "$TG_ARN" --query 'TargetHealthDescriptions[*].[Target.Id,TargetHealth.State,TargetHealth.Description]' --output table
          fi
        fi
        
        # Test basic connectivity to Load Balancer
        echo "Testing basic connectivity to Load Balancer..."
        curl -k -I --connect-timeout 10 https://$KUBERNETES_API_LB:6443/ || echo "Failed to connect to Load Balancer"
        
    - name: Install and configure ArgoCD
      run: |
        cd terraform
        export KUBECONFIG=./kubeconfig
        
        echo "Installing ArgoCD..."
        
        # Check if ArgoCD is already installed
        if kubectl get namespace argocd 2>/dev/null; then
          echo "ArgoCD namespace already exists. Checking if ArgoCD is already installed..."
          if kubectl get deployments -n argocd 2>/dev/null | grep -q argocd; then
            echo "ArgoCD is already installed. Skipping installation."
          else
            echo "ArgoCD namespace exists but no deployments found. Installing ArgoCD..."
            # Create ArgoCD namespace
            kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -
            
            # Install ArgoCD
            kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
          fi
        else
          echo "ArgoCD namespace does not exist. Installing ArgoCD..."
          # Create ArgoCD namespace
          kubectl create namespace argocd --dry-run=client -o yaml | kubectl apply -f -
          
          # Install ArgoCD
          kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
        fi
        
        # Wait for ArgoCD to be ready
        echo "Waiting for ArgoCD to be ready..."
        kubectl wait --for=condition=available --timeout=600s deployment/argocd-server -n argocd || {
          echo "ArgoCD server deployment failed to become ready. Checking logs..."
          kubectl logs -n argocd deployment/argocd-server --tail=20
          echo "Continuing with installation..."
        }
        
        # Scale down ArgoCD for resource efficiency
        echo "Scaling ArgoCD for resource efficiency..."
        kubectl scale deployment argocd-server -n argocd --replicas=1 || echo "argocd-server scaling failed (may not exist yet)"
        kubectl scale deployment argocd-repo-server -n argocd --replicas=1 || echo "argocd-repo-server scaling failed (may not exist yet)"
        kubectl scale deployment argocd-application-controller -n argocd --replicas=1 || echo "argocd-application-controller scaling failed (may not exist yet)"
        
        # Wait for scaled deployments to be ready (only if they exist)
        echo "Waiting for ArgoCD deployments to be ready..."
        kubectl wait --for=condition=available --timeout=300s deployment/argocd-server -n argocd || echo "argocd-server not found or not ready"
        kubectl wait --for=condition=available --timeout=300s deployment/argocd-repo-server -n argocd || echo "argocd-repo-server not found or not ready"
        kubectl wait --for=condition=available --timeout=300s deployment/argocd-application-controller -n argocd || echo "argocd-application-controller not found or not ready"
        
        # Check what ArgoCD deployments actually exist
        echo "Available ArgoCD deployments:"
        kubectl get deployments -n argocd || echo "No deployments found in argocd namespace"
        
        echo "ArgoCD installation completed successfully!"
        
        # Get ArgoCD admin password
        ARGOCD_PASSWORD=$(kubectl -n argocd get secret argocd-initial-admin-secret -o jsonpath="{.data.password}" | base64 -d 2>/dev/null || echo "password-not-available-yet")
        echo "ArgoCD admin password: $ARGOCD_PASSWORD"
        
        # Save ArgoCD info
        echo "ArgoCD admin password: $ARGOCD_PASSWORD" > argocd-info.txt
        echo "ArgoCD UI will be available at: http://$KUBERNETES_API_LB:8080" >> argocd-info.txt
        
    - name: Deploy ArgoCD Application
      run: |
        cd terraform
        export KUBECONFIG=./kubeconfig
        
        echo "Deploying ArgoCD Application..."
        
        # Apply the ArgoCD application manifest
        kubectl apply -f ../argocd/application.yaml
        
        # Wait for the application to be synced
        echo "Waiting for ArgoCD application to sync..."
        kubectl wait --for=condition=healthy --timeout=300s application/python-app -n argocd || {
          echo "Application sync failed. Checking ArgoCD application status..."
          kubectl describe application python-app -n argocd
          echo "Continuing..."
        }
        
        echo "ArgoCD application deployment completed!"
        
    - name: Deploy application with Helm
      run: |
        cd terraform
        export KUBECONFIG=./kubeconfig
        
        # Install Helm
        curl https://raw.githubusercontent.com/helm/helm/main/scripts/get-helm-3 | bash
        
        # Deploy the application
        echo "Deploying application with Helm..."
        helm upgrade --install python-app ../helm/python-app \
          --set image.repository=souravdixit04/demo_k8s_app_aws \
          --set image.tag=main \
          --set monitoring.serviceMonitor.enabled=false \
          --wait --timeout=300s
        
        echo "Application deployment completed!"
        
    - name: Verify deployment
      run: |
        cd terraform
        export KUBECONFIG=./kubeconfig
        
        echo "Verifying deployment..."
        kubectl get pods -A
        kubectl get svc -A
        
        # Check if ArgoCD is running
        echo "ArgoCD status:"
        kubectl get pods -n argocd
        
        # Check if application is running
        echo "Application status:"
        kubectl get pods -l app=python-app
        
    - name: Get cluster info
      run: |
        cd terraform
        terraform output -json > cluster-info.json
        
        # Add ArgoCD info to cluster info
        if [ -f argocd-info.txt ]; then
          echo "ArgoCD Information:" >> cluster-info.txt
          cat argocd-info.txt >> cluster-info.txt
        fi
        
    - name: Upload cluster info
      uses: actions/upload-artifact@v4
      with:
        name: cluster-info
        path: |
          terraform/cluster-info.json
          terraform/cluster-info.txt

  notify:
    name: Notify Deployment
    runs-on: ubuntu-latest
    needs: [deploy-infrastructure]
    if: always()
    
    steps:
    - name: Check for SLACK_WEBHOOK_URL secret
      run: |
        if [ -z "${{ secrets.SLACK_WEBHOOK_URL }}" ]; then
          echo "SLACK_WEBHOOK_URL is not set"
        else
          echo "SLACK_WEBHOOK_URL is set"
        fi 
    - name: Slack Notification
      uses: 8398a7/action-slack@v3
      with:
        status: success
        channel: '#new-channel'
        author_name: GitHub Actions
        github_token: ${{ secrets.GITHUB_TOKEN }}
      env:
        SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK_URL }}
        REGISTRY: ghcr.io
        IMAGE_NAME: souravdixit04/demo_k8s_app_aws

